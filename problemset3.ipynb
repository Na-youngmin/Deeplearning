{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading MNIST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1297)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3071)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.13), (0.3)),\n",
    "                              ])\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsample = images.reshape(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Constructing the network model\n",
    "\n",
    "`FNC` class를 정의하고 `model`을 생성하세요:\n",
    "\n",
    "- input layer\n",
    "  - number of input features를 hidden unit 128개로 선형변환 및 ReLU activation function\n",
    "- Hidden layer\n",
    "  - 128 hidden unit을 64개 hidden unit으로 선형변환 및 ReLU\n",
    "- Output layer\n",
    "  - 64개 hidden unit을 10개 class로 분류하기 위한 output layer\n",
    "  - 최종 layer의 activation은 없음 (linear layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN(nn.Module):\n",
    "    # 답작성\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.seq = nn.Sequential(nn.Linear(784,128),\n",
    "                                 nn.ReLU(True),\n",
    "                                 nn.Linear(128,64),\n",
    "                                 nn.ReLU(True),\n",
    "                                 nn.Linear(64,10)\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.seq(x) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions in PyTorch\n",
    "\n",
    "* 다음 과정으로, Pytorch에서 loss를 어떻게 연산하는지 배워보죠\n",
    "* `nn` module에서 다양한 loss function을 제공하는데, 예를 들면 `nn.CrossEntropyLoss`와 같은 함수가 있습니다\n",
    "    * 보통 관습적으로 loss function은 `critertion`이라는 변수로 받습니다 (`loss_function`등도 당연히 사용 가능합니다)\n",
    "* 지난 시간에 MNIST 문제는 확률 분포를 output으로 받는 것이 필요하다고 (또는 자연스러운 선택 임을) 학습했습니다 \n",
    "* 이런 확률 분포를 output으로 받는 경우 대응되는 좋은 loss function이 cross entropy입니다 (이론 강의에서 cross entropy가 무엇을 의미하는지 설명한 부분을 복습 해보세요)\n",
    "\n",
    "* Cross entropy의 정의는 \n",
    "\\begin{align*}\n",
    " J(\\theta) &=-\\frac{1}{m}\\sum_{i=1}^m P(y^{(i)}|x^{(i)})\\log(Q(y^{(i)}|x^{(i)}))\n",
    "\\end{align*}\n",
    "* 위 식은 두 확률 분포의 \"거리\"를 표현하는 식이라고 배웠습니다\n",
    "* 위에서 $P(y|x)$는 $y$의 label을 one hot coding 한 vector이고 $Q(y|x)$는 softmax를 취한 network output입니다\n",
    "* One hot coding은 label이 1이면 첫번째 자리만 '1'이고 나머지는 영인 벡터, label이 $k$이면 $k$ 번째 자리만 '1'이고 나머지는 0인 벡터입니다\n",
    "\n",
    "* 예를들어서 label이 2에 대한 one hot encoding\n",
    "\\begin{align*}\n",
    "y_\\textrm{one_hot}(2) &= \\begin{array}{cccccc}\n",
    "[0 & 0 & 1 & \\cdots & 0]\n",
    "\\end{array}\n",
    "\\end{align*}\n",
    "\n",
    "* 위 cross entropy 식에 대응 하는 방식은, label이 2라고 가정했을 때 분포는:\n",
    "\\begin{align*}\n",
    "P(y|x) = y_\\textrm{one_hot}(2), \\quad P(2|x) = (y_\\textrm{one_hot}(2))_2\n",
    "\\end{align*}\n",
    "\n",
    "* 또한, neural network의 마지막 linear layer의 output 값이 $z$라고 할때,\n",
    "\\begin{align*}\n",
    "Q(y=2|x) = \\sigma(z_2) = \\cfrac{\\exp(z_2)}{\\sum_k^K{\\exp(z_k)}}\n",
    "\\end{align*}\n",
    "\n",
    "![Classnote](https://drive.google.com/uc?export=download&id=17hcl4RJne65Vd17gKM8XKUTjYlqyFIY5)\n",
    "\n",
    "* pytorch에서 이를 수행하기 위해서 criterion을 `nn.CrossEntropyLoss`로 생성하고, network의 예측 값과, 실제 label 값을 입력으로 loss를 계산합니다\n",
    "  * 본 과정은 차근차근 설명하겠습니다\n",
    "* 그 전에 Pytorch에서 cross entropy 함수를 어떻게 적용하는지 먼저 이해할 필요가 있습니다 (중요합니다!!!)\n",
    "  * [Pytorch.org `nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)를 살펴보면\n",
    "\n",
    "> This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.\n",
    ">\n",
    "> The input is expected to contain scores for each class.\n",
    "\n",
    "* `nn.CrossEntropyLoss`는 `nn.LogSoftmax()`와 `nn.NLLLoss()` 하나의 class에서 수행한다고 되어 있습니다. \n",
    "* 두번째 줄에서 NLLLoss 는 negative log likelihood loss 입니다 \n",
    "\n",
    "* 이게 의미하는 바가 무엇이냐면, network의 output을 softmax function을 적용하여 출력하지 말고, softmax는 loss function에서 계산한다는 뜻입니다\n",
    "* 이렇게 구현한 이유는, 확률값이 작을 수 있어서 computation precision error를 방지하기 위해서 그냥 raw output 값을 받고, loss function에서 log(prob) 형태로 연산하도록 모듈을 구성하였습니다\n",
    "\n",
    "* 아래 코드를 보면 조금 더 이해가 될 것이라고 생각합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2840, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our log-probabilities\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logps and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Implement softmax\n",
    "\n",
    "다음 `softmax` 함수를 만드세요\n",
    "\n",
    "- `softmax(x)`\n",
    "- `input`: (batchsize, num_class)의 최종 linear layer output\n",
    "- `output`: `softmax` 취한 output이 (batchsize, softmaxoutput) 차원으로 정렬\n",
    "\n",
    "- 유용할 수 있는 함수:\n",
    "    - `torch.sum`\n",
    "    - `torch.exp`\n",
    "    - broadcasting 사용 (reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 답작성\n",
    "def softmax(x):    \n",
    "    return torch.exp(x)/torch.sum(torch.exp(x),axis = 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0967, 0.1151, 0.1096, 0.0954, 0.0952, 0.0943, 0.0877, 0.1019, 0.1021,\n",
       "         0.1020],\n",
       "        [0.0994, 0.1155, 0.1130, 0.1033, 0.0952, 0.0916, 0.0912, 0.0813, 0.1102,\n",
       "         0.0994],\n",
       "        [0.1116, 0.1114, 0.1144, 0.0913, 0.0984, 0.0872, 0.0917, 0.0909, 0.1027,\n",
       "         0.1003],\n",
       "        [0.1021, 0.0929, 0.1094, 0.0899, 0.1077, 0.0955, 0.0986, 0.1041, 0.1056,\n",
       "         0.0942],\n",
       "        [0.1126, 0.0961, 0.0966, 0.0893, 0.1069, 0.0965, 0.1034, 0.1002, 0.1021,\n",
       "         0.0964],\n",
       "        [0.1018, 0.0981, 0.0983, 0.0976, 0.1116, 0.0933, 0.0988, 0.1024, 0.0966,\n",
       "         0.1015],\n",
       "        [0.1046, 0.0900, 0.1027, 0.0918, 0.1136, 0.0863, 0.1039, 0.1020, 0.1068,\n",
       "         0.0982],\n",
       "        [0.1185, 0.0940, 0.1005, 0.0955, 0.1061, 0.0952, 0.0978, 0.0947, 0.1017,\n",
       "         0.0959],\n",
       "        [0.0945, 0.1003, 0.1002, 0.0934, 0.1079, 0.0976, 0.1014, 0.0991, 0.0991,\n",
       "         0.1065],\n",
       "        [0.1080, 0.0928, 0.1226, 0.1043, 0.0941, 0.1002, 0.1025, 0.0900, 0.1035,\n",
       "         0.0820],\n",
       "        [0.1046, 0.1008, 0.1034, 0.0932, 0.1114, 0.0938, 0.1030, 0.0985, 0.0985,\n",
       "         0.0928],\n",
       "        [0.0948, 0.1134, 0.1110, 0.1043, 0.0940, 0.0875, 0.1007, 0.0829, 0.1028,\n",
       "         0.1087],\n",
       "        [0.0975, 0.0941, 0.1042, 0.0970, 0.1072, 0.0951, 0.0931, 0.1065, 0.1082,\n",
       "         0.0970],\n",
       "        [0.1107, 0.1050, 0.1104, 0.0922, 0.0964, 0.0970, 0.0943, 0.1005, 0.0999,\n",
       "         0.0937],\n",
       "        [0.1043, 0.1206, 0.1127, 0.1041, 0.0832, 0.0850, 0.0897, 0.0904, 0.1127,\n",
       "         0.0973],\n",
       "        [0.1082, 0.0941, 0.1132, 0.0986, 0.1057, 0.0942, 0.1020, 0.1093, 0.0896,\n",
       "         0.0851],\n",
       "        [0.1050, 0.0944, 0.1058, 0.0955, 0.1123, 0.0870, 0.1042, 0.0991, 0.0999,\n",
       "         0.0969],\n",
       "        [0.1063, 0.1057, 0.1201, 0.0950, 0.1111, 0.0856, 0.1037, 0.0838, 0.0986,\n",
       "         0.0901],\n",
       "        [0.1016, 0.1057, 0.1253, 0.1010, 0.0938, 0.0907, 0.0915, 0.0975, 0.0986,\n",
       "         0.0942],\n",
       "        [0.1195, 0.0827, 0.1133, 0.0911, 0.1094, 0.1039, 0.1023, 0.0801, 0.1124,\n",
       "         0.0853],\n",
       "        [0.0927, 0.1100, 0.1093, 0.0997, 0.1021, 0.0933, 0.0995, 0.0899, 0.1011,\n",
       "         0.1024],\n",
       "        [0.1136, 0.0981, 0.0953, 0.0925, 0.0984, 0.0984, 0.0965, 0.1008, 0.1075,\n",
       "         0.0988],\n",
       "        [0.1021, 0.1049, 0.1163, 0.1010, 0.0930, 0.0996, 0.0868, 0.1052, 0.0961,\n",
       "         0.0951],\n",
       "        [0.1151, 0.1020, 0.1141, 0.1103, 0.1058, 0.0847, 0.0855, 0.0947, 0.0939,\n",
       "         0.0939],\n",
       "        [0.1025, 0.1004, 0.1196, 0.0914, 0.0927, 0.0934, 0.1000, 0.0988, 0.1067,\n",
       "         0.0945],\n",
       "        [0.1012, 0.0923, 0.0942, 0.0995, 0.1155, 0.0893, 0.1032, 0.1056, 0.1025,\n",
       "         0.0968],\n",
       "        [0.1032, 0.0940, 0.1129, 0.0902, 0.1080, 0.0942, 0.1016, 0.1009, 0.1028,\n",
       "         0.0921],\n",
       "        [0.1074, 0.0951, 0.1073, 0.1120, 0.1055, 0.0828, 0.1008, 0.1012, 0.0964,\n",
       "         0.0917],\n",
       "        [0.1125, 0.1039, 0.1102, 0.0991, 0.1122, 0.0859, 0.0996, 0.0858, 0.0929,\n",
       "         0.0981],\n",
       "        [0.1083, 0.1015, 0.1153, 0.0956, 0.1066, 0.0894, 0.1032, 0.0883, 0.0980,\n",
       "         0.0937],\n",
       "        [0.1031, 0.1017, 0.1133, 0.1089, 0.1090, 0.0867, 0.0972, 0.0946, 0.0952,\n",
       "         0.0904],\n",
       "        [0.1003, 0.1012, 0.1075, 0.0907, 0.1015, 0.0933, 0.1025, 0.0927, 0.1141,\n",
       "         0.0962],\n",
       "        [0.0997, 0.0959, 0.1108, 0.0962, 0.1002, 0.1012, 0.0939, 0.1001, 0.1073,\n",
       "         0.0948],\n",
       "        [0.1019, 0.0984, 0.1116, 0.1111, 0.1123, 0.0923, 0.0931, 0.0932, 0.0926,\n",
       "         0.0935],\n",
       "        [0.1005, 0.1142, 0.1085, 0.0895, 0.0993, 0.0912, 0.1066, 0.0913, 0.1031,\n",
       "         0.0958],\n",
       "        [0.0980, 0.1247, 0.1207, 0.0992, 0.0993, 0.0903, 0.0918, 0.0836, 0.1017,\n",
       "         0.0908],\n",
       "        [0.1220, 0.0903, 0.1009, 0.0915, 0.1051, 0.0900, 0.1036, 0.0976, 0.0996,\n",
       "         0.0994],\n",
       "        [0.1009, 0.1121, 0.1069, 0.0970, 0.0942, 0.0967, 0.0938, 0.0981, 0.1010,\n",
       "         0.0993],\n",
       "        [0.1049, 0.0988, 0.1028, 0.1030, 0.1066, 0.0924, 0.0975, 0.0994, 0.0997,\n",
       "         0.0949],\n",
       "        [0.1143, 0.0825, 0.1035, 0.0893, 0.1088, 0.0992, 0.1063, 0.0950, 0.1030,\n",
       "         0.0980],\n",
       "        [0.1008, 0.0944, 0.1110, 0.0912, 0.1066, 0.1002, 0.0980, 0.1061, 0.1013,\n",
       "         0.0905],\n",
       "        [0.1068, 0.0993, 0.1090, 0.0934, 0.1010, 0.0939, 0.0971, 0.1018, 0.1006,\n",
       "         0.0970],\n",
       "        [0.0976, 0.1111, 0.1058, 0.0957, 0.0985, 0.0986, 0.0956, 0.0991, 0.0993,\n",
       "         0.0988],\n",
       "        [0.1050, 0.0973, 0.1009, 0.1053, 0.0992, 0.0849, 0.1064, 0.1050, 0.0932,\n",
       "         0.1028],\n",
       "        [0.1018, 0.0958, 0.1085, 0.0952, 0.0980, 0.0982, 0.1009, 0.1039, 0.1100,\n",
       "         0.0877],\n",
       "        [0.1045, 0.1130, 0.1145, 0.0932, 0.0997, 0.0931, 0.0905, 0.0983, 0.1032,\n",
       "         0.0900],\n",
       "        [0.1119, 0.0992, 0.1004, 0.0959, 0.1006, 0.0906, 0.0962, 0.0999, 0.1042,\n",
       "         0.1009],\n",
       "        [0.1050, 0.0970, 0.1238, 0.0972, 0.1110, 0.0816, 0.0984, 0.0901, 0.1005,\n",
       "         0.0954],\n",
       "        [0.1007, 0.1021, 0.1187, 0.0990, 0.1074, 0.0857, 0.1010, 0.0830, 0.1013,\n",
       "         0.1013],\n",
       "        [0.1010, 0.1109, 0.1102, 0.0901, 0.0943, 0.0954, 0.1058, 0.0963, 0.0988,\n",
       "         0.0972],\n",
       "        [0.0978, 0.0915, 0.1202, 0.0890, 0.1044, 0.0937, 0.0973, 0.0846, 0.1110,\n",
       "         0.1106],\n",
       "        [0.1082, 0.1017, 0.1015, 0.0991, 0.1026, 0.0913, 0.0939, 0.0987, 0.1044,\n",
       "         0.0987],\n",
       "        [0.1005, 0.1019, 0.1243, 0.0981, 0.0971, 0.0874, 0.0983, 0.0928, 0.1012,\n",
       "         0.0985],\n",
       "        [0.1020, 0.1013, 0.1080, 0.0910, 0.0989, 0.0976, 0.1034, 0.0924, 0.1084,\n",
       "         0.0970],\n",
       "        [0.1012, 0.0915, 0.0985, 0.1054, 0.1155, 0.0960, 0.0987, 0.1002, 0.0962,\n",
       "         0.0968],\n",
       "        [0.1074, 0.0980, 0.1096, 0.0988, 0.1015, 0.0947, 0.0973, 0.0981, 0.1004,\n",
       "         0.0942],\n",
       "        [0.1025, 0.1062, 0.1117, 0.0976, 0.0998, 0.0959, 0.1052, 0.0973, 0.0899,\n",
       "         0.0941],\n",
       "        [0.1014, 0.1037, 0.1128, 0.1011, 0.0952, 0.0951, 0.0902, 0.0996, 0.1050,\n",
       "         0.0959],\n",
       "        [0.1122, 0.1035, 0.1032, 0.1069, 0.0968, 0.0933, 0.0867, 0.0950, 0.1052,\n",
       "         0.0971],\n",
       "        [0.1094, 0.0968, 0.1116, 0.0928, 0.1059, 0.1018, 0.0961, 0.1016, 0.0938,\n",
       "         0.0902],\n",
       "        [0.1072, 0.0958, 0.0956, 0.0914, 0.1064, 0.0985, 0.0982, 0.1002, 0.1055,\n",
       "         0.1013],\n",
       "        [0.1074, 0.0982, 0.1052, 0.0927, 0.1008, 0.0928, 0.0997, 0.1156, 0.0998,\n",
       "         0.0879],\n",
       "        [0.1015, 0.1182, 0.1104, 0.0969, 0.0944, 0.0894, 0.0942, 0.1046, 0.0996,\n",
       "         0.0906],\n",
       "        [0.1048, 0.1041, 0.1036, 0.0964, 0.0985, 0.0942, 0.1016, 0.1059, 0.0942,\n",
       "         0.0967]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = softmax(logits)\n",
    "ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Finding the Highest probability index\n",
    "\n",
    "Sample별 예측 확률값을 통하여 가장 높은 확률 값 예측 받는 함수를 작성하세요.\n",
    "\n",
    "`get_pred(ps)`\n",
    "- `input`: sample 별 확률값을 (batchsize, class probabilities) 로 받음\n",
    "- `output`: sample 별로 가장 높은 확률값의 index를 return\n",
    "\n",
    "- 유용할 수 있는 함수\n",
    "  - `torch.argmax`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(ps):\n",
    "    return torch.argmax(ps,axis =1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 2, 2, 0, 4, 4, 0, 4, 2, 4, 1, 8, 0, 1, 2, 4, 2, 2, 0, 1, 0, 2, 0,\n",
       "        2, 4, 2, 3, 0, 2, 2, 8, 2, 4, 1, 1, 0, 1, 4, 0, 2, 2, 1, 6, 8, 2, 0, 2,\n",
       "        2, 1, 2, 0, 2, 8, 4, 2, 2, 2, 0, 2, 0, 7, 1, 7])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pred(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-1.7774e-03, -1.7774e-03, -1.7774e-03,  ..., -1.7774e-03,\n",
      "         -1.7774e-03, -1.7774e-03],\n",
      "        [-3.0143e-03, -3.0143e-03, -3.0143e-03,  ..., -3.0143e-03,\n",
      "         -3.0143e-03, -3.0143e-03],\n",
      "        [-1.7573e-04, -1.7573e-04, -1.7573e-04,  ..., -1.7573e-04,\n",
      "         -1.7573e-04, -1.7573e-04],\n",
      "        ...,\n",
      "        [ 1.7473e-04,  1.7473e-04,  1.7473e-04,  ...,  1.7473e-04,\n",
      "          1.7473e-04,  1.7473e-04],\n",
      "        [ 2.9014e-03,  2.9014e-03,  2.9014e-03,  ...,  2.9014e-03,\n",
      "          2.9014e-03,  2.9014e-03],\n",
      "        [ 2.9927e-05,  2.9927e-05,  2.9927e-05,  ...,  2.9927e-05,\n",
      "          2.9927e-05,  2.9927e-05]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Before backward pass: \\n', model.seq[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model.seq[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network!\n",
    "\n",
    "\n",
    "* 자, 그럼 network parameter에 대한 loss function의 gradient를 구했으니, 이제 최적화를 할 수 있습니다\n",
    "* 최적화 기법은 SGD 이외에도 많으며 (SGD의 변형들임) [`optim` package](https://pytorch.org/docs/stable/optim.html)에서 찾아서 사용할 수 있습니다\n",
    "* 예를 들어서 SGD는 `optim.SGD`를 통해서 불러올 수 있습니다\n",
    "* 아래 예를 보죠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* model.parameters()는 우리 network의 모든 training parameter이며, lr는 learning rate 입니다\n",
    "* 자, 이제 traning에 필요한 모든 부분이 준비되었습니다\n",
    "* 전체 데이터에 대한 training은 숙제로 하고, 한 batch만 수행하는 과정을 살펴보죠\n",
    "* Pytorch에서 training의 전체 흐름은 다음과 같습니다:\n",
    "\n",
    "1. Network에서 forward pass\n",
    "2. Foward pass를 통해서 얻은 output을 활용하여 loss를 구한다\n",
    "3. Gradient를 구하기 위해서 `loss.backward()`를 실행한다\n",
    "4. Optimizer에서 weight를 한번 update 한다 (SGD의 경우 gradient에 대해서 한번 update)\n",
    "\n",
    "**[중요]**\n",
    "* 한가지 주의할 점은, 한 Parameter들에 대해서 gradient를 여러개 구해야하는 경우, (예, batch 처리) gradient 값들은 계속 추가적으로 저장됩니다\n",
    "* 한번 parameter update가 끝났으면, gradient 값을 초기화해야, 새로운 batch에 대한 새 gradient 값을 계산 합니다\n",
    "* 이를 위해서 batch 시작시에 `optimizer.zero_grad()`를 실행해줘야합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[-0.0160, -0.0313,  0.0356,  ..., -0.0100,  0.0210,  0.0009],\n",
      "        [ 0.0113, -0.0188,  0.0309,  ..., -0.0321, -0.0245, -0.0084],\n",
      "        [ 0.0056, -0.0343, -0.0346,  ...,  0.0151,  0.0254,  0.0028],\n",
      "        ...,\n",
      "        [-0.0232,  0.0124,  0.0334,  ..., -0.0308, -0.0300, -0.0077],\n",
      "        [ 0.0301, -0.0242,  0.0058,  ..., -0.0193,  0.0023,  0.0051],\n",
      "        [-0.0310, -0.0319,  0.0325,  ...,  0.0286, -0.0270,  0.0110]],\n",
      "       requires_grad=True)\n",
      "Gradient - Parameter containing:\n",
      "tensor([[-0.0160, -0.0312,  0.0356,  ..., -0.0099,  0.0210,  0.0010],\n",
      "        [ 0.0114, -0.0186,  0.0310,  ..., -0.0320, -0.0244, -0.0083],\n",
      "        [ 0.0056, -0.0343, -0.0345,  ...,  0.0151,  0.0254,  0.0028],\n",
      "        ...,\n",
      "        [-0.0232,  0.0124,  0.0333,  ..., -0.0308, -0.0301, -0.0077],\n",
      "        [ 0.0300, -0.0243,  0.0057,  ..., -0.0193,  0.0022,  0.0051],\n",
      "        [-0.0310, -0.0319,  0.0325,  ...,  0.0287, -0.0270,  0.0110]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model.seq[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images = images.reshape(images.shape[0], -1)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumulated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "\n",
    "print('Gradient -', model.seq[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Starting epoch: 0 --------\n",
      "Training loss: 0.747716810387462\n",
      "Training acc: 0.8139658570289612\n",
      "------ Starting epoch: 1 --------\n",
      "Training loss: 0.310067354520755\n",
      "Training acc: 0.9109141826629639\n",
      "------ Starting epoch: 2 --------\n",
      "Training loss: 0.2538989438558184\n",
      "Training acc: 0.9264225959777832\n",
      "------ Starting epoch: 3 --------\n",
      "Training loss: 0.2164201992414971\n",
      "Training acc: 0.9378165006637573\n",
      "------ Starting epoch: 4 --------\n",
      "Training loss: 0.18855808418331496\n",
      "Training acc: 0.9463952779769897\n",
      "------ Starting epoch: 5 --------\n",
      "Training loss: 0.16574235087725273\n",
      "Training acc: 0.9528584480285645\n",
      "------ Starting epoch: 6 --------\n",
      "Training loss: 0.14806592804210972\n",
      "Training acc: 0.957722544670105\n",
      "------ Starting epoch: 7 --------\n",
      "Training loss: 0.13349785508790504\n",
      "Training acc: 0.961670458316803\n",
      "------ Starting epoch: 8 --------\n",
      "Training loss: 0.12140391498748491\n",
      "Training acc: 0.9651519060134888\n",
      "------ Starting epoch: 9 --------\n",
      "Training loss: 0.11101568156261561\n",
      "Training acc: 0.9685667753219604\n",
      "------ Starting epoch: 10 --------\n",
      "Training loss: 0.10178435583318919\n",
      "Training acc: 0.970649003982544\n",
      "------ Starting epoch: 11 --------\n",
      "Training loss: 0.09415966593451909\n",
      "Training acc: 0.9731476306915283\n",
      "------ Starting epoch: 12 --------\n",
      "Training loss: 0.08707005754192627\n",
      "Training acc: 0.974880039691925\n",
      "------ Starting epoch: 13 --------\n",
      "Training loss: 0.08096447583935312\n",
      "Training acc: 0.9767457246780396\n",
      "------ Starting epoch: 14 --------\n",
      "Training loss: 0.07541725295149028\n",
      "Training acc: 0.9788779020309448\n",
      "------ Starting epoch: 15 --------\n",
      "Training loss: 0.07003872888659173\n",
      "Training acc: 0.9800606369972229\n",
      "------ Starting epoch: 16 --------\n",
      "Training loss: 0.06572688659573638\n",
      "Training acc: 0.9813099503517151\n",
      "------ Starting epoch: 17 --------\n",
      "Training loss: 0.06151544364657738\n",
      "Training acc: 0.9828758239746094\n",
      "------ Starting epoch: 18 --------\n",
      "Training loss: 0.05787613738014468\n",
      "Training acc: 0.9839918613433838\n",
      "------ Starting epoch: 19 --------\n",
      "Training loss: 0.05438088469389977\n",
      "Training acc: 0.9851245880126953\n",
      "------ Starting epoch: 20 --------\n",
      "Training loss: 0.051171106909876314\n",
      "Training acc: 0.986190676689148\n",
      "------ Starting epoch: 21 --------\n",
      "Training loss: 0.04792288580645265\n",
      "Training acc: 0.9869236350059509\n",
      "------ Starting epoch: 22 --------\n",
      "Training loss: 0.04520560805745391\n",
      "Training acc: 0.9875233173370361\n",
      "------ Starting epoch: 23 --------\n",
      "Training loss: 0.04289514125326772\n",
      "Training acc: 0.9886727333068848\n",
      "------ Starting epoch: 24 --------\n",
      "Training loss: 0.040448170846332114\n",
      "Training acc: 0.9893056750297546\n",
      "------ Starting epoch: 25 --------\n",
      "Training loss: 0.03806553144446775\n",
      "Training acc: 0.9902052283287048\n",
      "------ Starting epoch: 26 --------\n",
      "Training loss: 0.036078402173093785\n",
      "Training acc: 0.9908548593521118\n",
      "------ Starting epoch: 27 --------\n",
      "Training loss: 0.03442774818135874\n",
      "Training acc: 0.9912213683128357\n",
      "------ Starting epoch: 28 --------\n",
      "Training loss: 0.03247361660906191\n",
      "Training acc: 0.9921208620071411\n",
      "------ Starting epoch: 29 --------\n",
      "Training loss: 0.030750343798727257\n",
      "Training acc: 0.9925373196601868\n",
      "------ Starting epoch: 30 --------\n",
      "Training loss: 0.029045879510097277\n",
      "Training acc: 0.9931703209877014\n",
      "------ Starting epoch: 31 --------\n",
      "Training loss: 0.027467701043228685\n",
      "Training acc: 0.993786633014679\n",
      "------ Starting epoch: 32 --------\n",
      "Training loss: 0.02608843688440543\n",
      "Training acc: 0.9942697286605835\n",
      "------ Starting epoch: 33 --------\n",
      "Training loss: 0.024846870640068373\n",
      "Training acc: 0.9944196343421936\n",
      "------ Starting epoch: 34 --------\n",
      "Training loss: 0.023438686943689446\n",
      "Training acc: 0.9950360059738159\n",
      "------ Starting epoch: 35 --------\n",
      "Training loss: 0.022239064882029848\n",
      "Training acc: 0.9955690503120422\n",
      "------ Starting epoch: 36 --------\n",
      "Training loss: 0.021146203142909537\n",
      "Training acc: 0.9956856369972229\n",
      "------ Starting epoch: 37 --------\n",
      "Training loss: 0.020168134644041772\n",
      "Training acc: 0.9960354566574097\n",
      "------ Starting epoch: 38 --------\n",
      "Training loss: 0.01913237114246291\n",
      "Training acc: 0.996268630027771\n",
      "------ Starting epoch: 39 --------\n",
      "Training loss: 0.018184691301059486\n",
      "Training acc: 0.9964185953140259\n",
      "------ Starting epoch: 40 --------\n",
      "Training loss: 0.017188700844100846\n",
      "Training acc: 0.9969516396522522\n",
      "------ Starting epoch: 41 --------\n",
      "Training loss: 0.016415798392614474\n",
      "Training acc: 0.9973347783088684\n",
      "------ Starting epoch: 42 --------\n",
      "Training loss: 0.015640581044979566\n",
      "Training acc: 0.9975513219833374\n",
      "------ Starting epoch: 43 --------\n",
      "Training loss: 0.014816449703038064\n",
      "Training acc: 0.9978345036506653\n",
      "------ Starting epoch: 44 --------\n",
      "Training loss: 0.014108215611843047\n",
      "Training acc: 0.9979011416435242\n",
      "------ Starting epoch: 45 --------\n",
      "Training loss: 0.013423409753492467\n",
      "Training acc: 0.9980843663215637\n",
      "------ Starting epoch: 46 --------\n",
      "Training loss: 0.012837514380250733\n",
      "Training acc: 0.9982842206954956\n",
      "------ Starting epoch: 47 --------\n",
      "Training loss: 0.01216241228903062\n",
      "Training acc: 0.9985840916633606\n",
      "------ Starting epoch: 48 --------\n",
      "Training loss: 0.011566009499189227\n",
      "Training acc: 0.9985008239746094\n",
      "------ Starting epoch: 49 --------\n",
      "Training loss: 0.011131568484121005\n",
      "Training acc: 0.9985840916633606\n",
      "------ Starting epoch: 50 --------\n",
      "Training loss: 0.01061021644216992\n",
      "Training acc: 0.9987840056419373\n",
      "------ Starting epoch: 51 --------\n",
      "Training loss: 0.010061446794286133\n",
      "Training acc: 0.9989005923271179\n",
      "------ Starting epoch: 52 --------\n",
      "Training loss: 0.00960892396809003\n",
      "Training acc: 0.9989672303199768\n",
      "------ Starting epoch: 53 --------\n",
      "Training loss: 0.009311190634436691\n",
      "Training acc: 0.9989339113235474\n",
      "------ Starting epoch: 54 --------\n",
      "Training loss: 0.008826765291485142\n",
      "Training acc: 0.9991171360015869\n",
      "------ Starting epoch: 55 --------\n",
      "Training loss: 0.008414327892770709\n",
      "Training acc: 0.9991670846939087\n",
      "------ Starting epoch: 56 --------\n",
      "Training loss: 0.008071085496867416\n",
      "Training acc: 0.9992170929908752\n",
      "------ Starting epoch: 57 --------\n",
      "Training loss: 0.007658693071762308\n",
      "Training acc: 0.9993003606796265\n",
      "------ Starting epoch: 58 --------\n",
      "Training loss: 0.0073389152534304065\n",
      "Training acc: 0.9993170499801636\n",
      "------ Starting epoch: 59 --------\n",
      "Training loss: 0.00708386803759254\n",
      "Training acc: 0.9994336366653442\n",
      "------ Starting epoch: 60 --------\n",
      "Training loss: 0.006875985994317464\n",
      "Training acc: 0.9994170069694519\n",
      "------ Starting epoch: 61 --------\n",
      "Training loss: 0.006534535549909322\n",
      "Training acc: 0.9995002746582031\n",
      "------ Starting epoch: 62 --------\n",
      "Training loss: 0.006277415179380668\n",
      "Training acc: 0.9995002746582031\n",
      "------ Starting epoch: 63 --------\n",
      "Training loss: 0.00602756864100229\n",
      "Training acc: 0.999566912651062\n",
      "------ Starting epoch: 64 --------\n",
      "Training loss: 0.005793605450781431\n",
      "Training acc: 0.9996168613433838\n",
      "------ Starting epoch: 65 --------\n",
      "Training loss: 0.005569204901479609\n",
      "Training acc: 0.9996834993362427\n",
      "------ Starting epoch: 66 --------\n",
      "Training loss: 0.005333629646040658\n",
      "Training acc: 0.9997168183326721\n",
      "------ Starting epoch: 67 --------\n",
      "Training loss: 0.005157958053917846\n",
      "Training acc: 0.9997667670249939\n",
      "------ Starting epoch: 68 --------\n",
      "Training loss: 0.004984508371825811\n",
      "Training acc: 0.9997168183326721\n",
      "------ Starting epoch: 69 --------\n",
      "Training loss: 0.004806822385335627\n",
      "Training acc: 0.9997334480285645\n",
      "------ Starting epoch: 70 --------\n",
      "Training loss: 0.004667891075197078\n",
      "Training acc: 0.999783456325531\n",
      "------ Starting epoch: 71 --------\n",
      "Training loss: 0.004426471240434876\n",
      "Training acc: 0.9998000860214233\n",
      "------ Starting epoch: 72 --------\n",
      "Training loss: 0.004293173411004702\n",
      "Training acc: 0.9998167753219604\n",
      "------ Starting epoch: 73 --------\n",
      "Training loss: 0.004138528562063646\n",
      "Training acc: 0.9998334050178528\n",
      "------ Starting epoch: 74 --------\n",
      "Training loss: 0.003991340378444627\n",
      "Training acc: 0.9998000860214233\n",
      "------ Starting epoch: 75 --------\n",
      "Training loss: 0.003877740424352391\n",
      "Training acc: 0.9998500943183899\n",
      "------ Starting epoch: 76 --------\n",
      "Training loss: 0.0037627647444425866\n",
      "Training acc: 0.9998500943183899\n",
      "------ Starting epoch: 77 --------\n",
      "Training loss: 0.003654717989049967\n",
      "Training acc: 0.9998500943183899\n",
      "------ Starting epoch: 78 --------\n",
      "Training loss: 0.003536547297241488\n",
      "Training acc: 0.9998500943183899\n",
      "------ Starting epoch: 79 --------\n",
      "Training loss: 0.003413658895021296\n",
      "Training acc: 0.9998834133148193\n",
      "------ Starting epoch: 80 --------\n",
      "Training loss: 0.0033416393324704236\n",
      "Training acc: 0.9999500513076782\n",
      "------ Starting epoch: 81 --------\n",
      "Training loss: 0.003228499528561989\n",
      "Training acc: 0.9999333620071411\n",
      "------ Starting epoch: 82 --------\n",
      "Training loss: 0.003142374904095833\n",
      "Training acc: 0.9999167323112488\n",
      "------ Starting epoch: 83 --------\n",
      "Training loss: 0.003057125739705526\n",
      "Training acc: 0.9998834133148193\n",
      "------ Starting epoch: 84 --------\n",
      "Training loss: 0.0029578734452416476\n",
      "Training acc: 0.9999167323112488\n",
      "------ Starting epoch: 85 --------\n",
      "Training loss: 0.0028917231910198286\n",
      "Training acc: 0.9999333620071411\n",
      "------ Starting epoch: 86 --------\n",
      "Training loss: 0.0028099631118611856\n",
      "Training acc: 0.9999500513076782\n",
      "------ Starting epoch: 87 --------\n",
      "Training loss: 0.0027324747909184374\n",
      "Training acc: 0.9999167323112488\n",
      "------ Starting epoch: 88 --------\n",
      "Training loss: 0.002677260625375887\n",
      "Training acc: 0.9999666810035706\n",
      "------ Starting epoch: 89 --------\n",
      "Training loss: 0.002600270311478704\n",
      "Training acc: 0.9999333620071411\n",
      "------ Starting epoch: 90 --------\n",
      "Training loss: 0.0025331907816465073\n",
      "Training acc: 0.9999500513076782\n",
      "------ Starting epoch: 91 --------\n",
      "Training loss: 0.002482571542702505\n",
      "Training acc: 0.9999666810035706\n",
      "------ Starting epoch: 92 --------\n",
      "Training loss: 0.002413287173236732\n",
      "Training acc: 0.9999500513076782\n",
      "------ Starting epoch: 93 --------\n",
      "Training loss: 0.002355557739709083\n",
      "Training acc: 0.9999333620071411\n",
      "------ Starting epoch: 94 --------\n",
      "Training loss: 0.002293441154512399\n",
      "Training acc: 0.9999833703041077\n",
      "------ Starting epoch: 95 --------\n",
      "Training loss: 0.002259080312292671\n",
      "Training acc: 0.9999833703041077\n",
      "------ Starting epoch: 96 --------\n",
      "Training loss: 0.0021966183509005343\n",
      "Training acc: 1.0\n",
      "------ Starting epoch: 97 --------\n",
      "Training loss: 0.0021620751327696267\n",
      "Training acc: 0.9999500513076782\n",
      "------ Starting epoch: 98 --------\n",
      "Training loss: 0.002099583795672112\n",
      "Training acc: 0.9999833703041077\n",
      "------ Starting epoch: 99 --------\n",
      "Training loss: 0.0020535727756751106\n",
      "Training acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 100\n",
    "loss_epoch = []\n",
    "acc_epoch = []\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"------ Starting epoch: {e} --------\")\n",
    "    running_loss = 0\n",
    "    running_acc = 0\n",
    "\n",
    "    for images, labels in trainloader:\n",
    "        images = images.reshape(images.shape[0], -1)\n",
    "    \n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        ps = softmax(logits)\n",
    "        pred = get_pred(ps)\n",
    "        running_acc += torch.sum(pred==labels)/labels.shape[0] \n",
    "        \n",
    "               \n",
    "    acc_epoch.append(running_acc/len(trainloader))    \n",
    "    loss_epoch.append(running_loss/len(trainloader))\n",
    "    print(f\"Training loss: {loss_epoch[e]}\")\n",
    "    print(f\"Training acc: {acc_epoch[e]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAAsTAAALEwEAmpwYAAAieUlEQVR4nO3de3hVd53v8fd3752dhAQSriklFKjQC0V6IaX1Uhts60OdDvXRqnjpVKeV8Ywctc7jTPvodLRz5jnqzNEzo5wqpzq2Xoq1VodRjq2tbJxqbaEX2wJCA70AlkK5byD37/ljrSQ7ISlJyGKT/D6v59lP9lrrt9f+fVmQD791NXdHRETClSp2B0REpLgUBCIigVMQiIgETkEgIhI4BYGISOAyxe7AQE2YMMGnT58+qM8ePnyYioqKoe3QMBBi3SHWDGHWHWLNMPC6n3jiidfcfWJvy4ZdEEyfPp1169YN6rO5XI76+vqh7dAwEGLdIdYMYdYdYs0w8LrN7KW+lmnXkIhI4BQEIiKBUxCIiAROQSAiEjgFgYhI4BQEIiKBUxCIiAQumCBY++JefrK5mda29mJ3RUTklBJMEDz18j7+c2sLja0KAhGRQsEEQTYdldqsIBAR6SaYICjJRKW2aNeQiEg3wQSBRgQiIr0LJwjiEUGTgkBEpJtggqA0oxGBiEhvggmCko5dQzpGICLSTTBBkNXBYhGRXoUTBDpYLCLSq3CCQMcIRER6lWgQmNlCM9tkZg1mdksvy79mZk/Hr81mtj+pvuisIRGR3iX2zGIzSwPLgKuA7cBaM1vp7hs62rj7zQXt/ztwYVL9yepgsYhIr5IcEcwHGtx9q7s3AyuAa1+n/QeAe5LqTOfBYo0IRES6SWxEAEwBthVMbwcu6a2hmU0DZgC/7mP5EmAJQE1NDblcbsCd2dsYBcCzGzYy/lDDgD8/nOXz+UH9mQ1nIdYMYdYdYs0wtHUnGQQDsRi4z93belvo7suB5QB1dXVeX18/4C/Yk2+C3EPMeMMs6t88/QS6OvzkcjkG82c2nIVYM4RZd4g1w9DWneSuoR3A1ILp2nhebxaT4G4h0FlDIiJ9STII1gKzzGyGmWWJftmv7NnIzM4BxgKPJtgXXVksItKHxILA3VuBpcADwEbgXndfb2a3m9migqaLgRXu7kn1BXRBmYhIXxI9RuDuq4BVPebd1mP6C0n2oUMqZaRNIwIRkZ6CubIYoCSlEYGISE9BBUFaQSAicoyggqAkZbr7qIhID0EFQUYjAhGRYwQXBE0aEYiIdBNUEJSkTCMCEZEeggqCjGnXkIhIT2EFQUqPqhQR6Sm4INCIQESku6CCoCRlurJYRKSHoIJAF5SJiBwrqCAoSeleQyIiPQUVBBmdPioicozAgkC7hkREegoqCLRrSETkWEEFgS4oExE5VlhBoLuPiogcI9EgMLOFZrbJzBrM7JY+2rzPzDaY2Xoz+2GS/YmuLHba2xN9KqaIyLCS2KMqzSwNLAOuArYDa81spbtvKGgzC7gVeIu77zOzSUn1B6JjBBAdJyhLpZP8KhGRYSPJEcF8oMHdt7p7M7ACuLZHm48By9x9H4C770qwP2RSBuiAsYhIoSQfXj8F2FYwvR24pEebswDM7LdAGviCu/+y54rMbAmwBKCmpoZcLjeoDrW1NAFGbs0jjCm1Qa1jOMrn84P+MxuuQqwZwqw7xJphaOtOMgj6+/2zgHqgFviNmb3R3fcXNnL35cBygLq6Oq+vrx/Ul+W2/Qpo5uJLL2VyVfngez3M5HI5BvtnNlyFWDOEWXeINcPQ1p3krqEdwNSC6dp4XqHtwEp3b3H3F4DNRMGQiM5jBDqFVESkU5JBsBaYZWYzzCwLLAZW9mjzM6LRAGY2gWhX0dakOtR5jEBBICLSKbEgcPdWYCnwALARuNfd15vZ7Wa2KG72ALDHzDYAq4HPuvuepPqUiattUhCIiHRK9BiBu68CVvWYd1vBewc+E78S1xEEuqhMRKRLUFcWl2jXkIjIMYIKgkzBBWUiIhIJMwg0IhAR6RRWEMTXkCkIRES6hBUEusWEiMgxggoCXVAmInKsoIJAB4tFRI4VWBDo9FERkZ4CC4Lop4JARKRLUEFQoiuLRUSOEVQQpHX6qIjIMYIKAjMjm0nRpBGBiEinoIIAIJtOaUQgIlIgvCDIpHSMQESkQHhBoBGBiEg34QVBRkEgIlIozCDQriERkU7BBUGJdg2JiHSTaBCY2UIz22RmDWZ2Sy/LP2Jmu83s6fh1U5L9gY4RgSf9NSIiw0Zizyw2szSwDLgK2A6sNbOV7r6hR9MfufvSpPrRU2k6RXNr28n6OhGRU16SI4L5QIO7b3X3ZmAFcG2C39cvOlgsItJdYiMCYAqwrWB6O3BJL+3eY2ZvAzYDN7v7tp4NzGwJsASgpqaGXC43qA7l83kOHmjkQJMPeh3DUT6fD6peCLNmCLPuEGuGoa07ySDoj/8E7nH3JjP7K+Au4O09G7n7cmA5QF1dndfX1w/qy3K5HJMnVdL42mHq6y8ffK+HmVwux2D/zIarEGuGMOsOsWYY2rqT3DW0A5haMF0bz+vk7nvcvSmevBOYl2B/AMhm0rToYLGISKckg2AtMMvMZphZFlgMrCxsYGaTCyYXARsT7A+gK4tFRHpKbNeQu7ea2VLgASANfMfd15vZ7cA6d18JfNLMFgGtwF7gI0n1p0M2k6JJQSAi0inRYwTuvgpY1WPebQXvbwVuTbIPPWXTptNHRUQKBHdlcXT3UR0jEBHpEGQQ6F5DIiJdwguCdJq2dqetXaMCEREIMAhKMtGDi3XmkIhIJLggyKajkhUEIiKR4IKgNBMHgY4TiIgAAQZBVkEgItJNuEGgXUMiIkCAQVCiYwQiIt0EFwQ6WCwi0l14QaBjBCIi3YQbBBoRiIgAAQaBTh8VEekuuCDQwWIRke6CCwLtGhIR6S68IIhHBC3aNSQiAoQYBBoRiIh0k2gQmNlCM9tkZg1mdsvrtHuPmbmZ1SXZH+gaETRpRCAiAiQYBGaWBpYBVwOzgQ+Y2exe2o0GPgU8llRfCmlEICLSXZIjgvlAg7tvdfdmYAVwbS/t/hH4MtCYYF86dQSBjhGIiESSfHj9FGBbwfR24JLCBmZ2ETDV3X9hZp/ta0VmtgRYAlBTU0MulxtUh/L5PI8+8l8AbHp+CznfdpxPjAz5fH7Qf2bDVYg1Q5h1h1gzDG3d/QoCM/sU8O/AIeBO4ELgFnd/cLBfbGYp4KvAR47X1t2XA8sB6urqvL6+flDfmcvlqK+vJ/WrXzBl6jTq688e1HqGm466QxJizRBm3SHWDENbd393Df2lux8E3gGMBa4HvnScz+wAphZM18bzOowG5gA5M3sRuBRYeTIOGJek9QB7EZEO/Q0Ci3++E/ieu68vmNeXtcAsM5thZllgMbCyY6G7H3D3Ce4+3d2nA78HFrn7ugFVMAjZTEoHi0VEYv0NgifM7EGiIHggPtPndX+TunsrsBR4ANgI3Ovu683sdjNbdCKdPlGlGY0IREQ69Pdg8Y3ABcBWdz9iZuOAjx7vQ+6+CljVY95tfbSt72dfTlg2rRGBiEiH/o4I3gRscvf9ZvZh4PPAgeS6lSztGhIR6dLfILgDOGJm5wN/A2wB7k6sVwkr0YhARKRTf4Og1d2d6IKwb7j7MqKzfoalrI4RiIh06u8xgkNmdivRaaOXxdcAlCTXrWRlMyldWSwiEuvviOD9QBPR9QQ7ia4J+OfEepWwbDpFk3YNiYgA/QyC+Jf/D4AqM7sGaHT3YXuMQAeLRUS69CsIzOx9wOPAe4H3AY+Z2XVJdixJOn1URKRLf48RfA642N13AZjZROAh4L6kOpYkHSMQEenS32MEqY4QiO0ZwGdPOTprSESkS39HBL80sweAe+Lp99PjiuHhRLuGRES69CsI3P2zZvYe4C3xrOXu/tPkupWsEh0sFhHp1O8H07j7T4CfJNiXk0YjAhGRLq8bBGZ2CPDeFgHu7mMS6VXCdPdREZEurxsE7j5sbyPxejoOFrs7Zsd7rIKIyMg2bM/8OREl6RTu0Nre22BHRCQsQQZBNhOVreMEIiKhBkFaQSAi0iHMIIhHBLq6WEQk4SAws4VmtsnMGszsll6Wf9zMnjWzp83sETObnWR/OlSVR3fQ3p1vOhlfJyJySkssCMwsDSwDrgZmAx/o5Rf9D939je5+AfAV4KtJ9afQeadHZ70+u33YPm1TRGTIJDkimA80uPtWd28GVhA94ayTux8smKyg92sWhtz08RWMLsvwBwWBiAgWPYEygRVHt6le6O43xdPXA5e4+9Ie7T4BfAbIAm939+d7WdcSYAlATU3NvBUrVgyqT/l8nsrKSgC+svYoh1vgi28uH9S6hpPCukMRYs0QZt0h1gwDr3vBggVPuHtdrwvdPZEXcB1wZ8H09UTPO+6r/QeBu4633nnz5vlgrV69uvP9l/7fRn/Drb/wo82tg17fcFFYdyhCrNk9zLpDrNl94HUD67yP36tJ7hraAUwtmK6N5/VlBfCuBPvTzfm1VbS2OxtfOXj8xiIiI1iSQbAWmGVmM8wsCywGVhY2MLNZBZN/BhyzWygpc2urAXhGxwlEJHD9vvvoQLl7q5ktBR4A0sB33H29md1ONERZCSw1syuBFmAfcENS/elpclUZEypL+cP2/SfrK0VETkmJBQGAu6+ixwNs3P22gvefSvL7X4+ZMbe2SiMCEQlekFcWd5hbW8WW3XnyTa3F7oqISNEEHQTn11bjDs/t0KhARMIVdBDMra0C4BkdJxCRgAUdBOMrS5lSXa4rjEUkaEEHAcD5U6s0IhCRoAUfBHNrq9m29yh7DzcXuysiIkURfBDUTRsLwKNb9hS5JyIixRF8EFx4xljGVWR5cMPOYndFRKQogg+CdMp4+zmTWP3HXXpimYgEKfggAHjH7BoONrby+At7i90VEZGTTkEAXDZrImUlKX614dVid0VE5KRTEADl2TRvnTmRB9fv7Hg2gohIMBQEsXfMruFPBxpZ/yc9n0BEwqIgiF1x7iRShnYPiUhwFASx8ZWlzJs2VkEgIsFREBS4anYNG145yLa9R4rdFRGRk0ZBUODqOZMxg/ue2F7sroiInDSJBoGZLTSzTWbWYGa39LL8M2a2wcyeMbOHzWxakv05nqnjRnH5WRO55/GXdXGZiAQjsSAwszSwDLgamA18wMxm92j2FFDn7nOB+4CvJNWf/vqLN01j16EmHlyvYwUiEoYkRwTzgQZ33+ruzcAK4NrCBu6+2t07dsj/HqhNsD/9cvlZk6gdW87dj75Y7K6IiJwUltQFVGZ2HbDQ3W+Kp68HLnH3pX20/waw093/Ry/LlgBLAGpqauatWLFiUH3K5/NUVlYet92qrc3cu7mFf3pLOVNGD//DKP2teyQJsWYIs+4Qa4aB171gwYIn3L2ut2WZIevVCTCzDwN1wOW9LXf35cBygLq6Oq+vrx/U9+RyOfrz2bkXN/Oz//kwf2ybxIfq5wzqu04l/a17JAmxZgiz7hBrhqGtO8n/7u4AphZM18bzujGzK4HPAYvcvSnB/vTbuIos18ydzP1Pbiff1Frs7oiIJCrJIFgLzDKzGWaWBRYDKwsbmNmFwLeIQmBXgn0ZsL9403QON7dxz2MvF7srIiKJSiwI3L0VWAo8AGwE7nX39WZ2u5ktipv9M1AJ/NjMnjazlX2s7qS7YGo1b505gW+u2cKRZo0KRGTkSvQYgbuvAlb1mHdbwfsrk/z+E3XzVbN4zx2PcvejL/Hxy99Q7O6IiCRi+J8Sk6B508Zx+VkT+daaLTpWICIjloLgOG6+6iz2HWnhu799odhdERFJhILgOC6YWs0V50xi+W+2crCxpdjdEREZcgqCfrj5qrM42NjKvz30fLG7IiIy5BQE/TBnShUfuuQMvvPbF3jy5X3F7o6IyJBSEPTTLVefw2ljyvjb+56hqbWt2N0RERkyCoJ+Gl1Wwj+9+4007Mrz9Ycbit0dEZEhoyAYgAVnT+I9F9Vyx5otPLfjQLG7IyIyJBQEA/T315zLhMosf/2DJ9l/pLnY3REROWEKggGqHpXl/3xoHq8cOMqnVjxNW3syt/EWETlZFASDMG/aWL64aA5rNu/mq7/aVOzuiIickFPieQTD0QcvOYNnd+xn2eotnDt5DNfMPb3YXRIRGRQFwQn4wqLzeP7VPDf/6GkqshkWnDOp2F0SERkw7Ro6AaWZNN/56MWcc9oY/ur7T/C7hteK3SURkQFTEJygMWUl3P2X85kxvoIb71rH2hf3FrtLIiIDoiAYAmMrsnzvpvlMri7j+m8/xkMbXi12l0RE+k1BMEQmjS7j3r96E2fXjGbJ99ax4nE94lJEhgcFwRCaUFnKDz92KZfNmsgt9z/L/3pwE+26zkBETnGJBoGZLTSzTWbWYGa39LL8bWb2pJm1mtl1SfblZKkozXDnDXW8r66Wr/+6gY/dvU7PMRCRU1piQWBmaWAZcDUwG/iAmc3u0exl4CPAD5PqRzGUpFN8+T1z+eKi81izeTfv+sZv2fzqoWJ3S0SkV0mOCOYDDe6+1d2bgRXAtYUN3P1Fd38GaE+wH0VhZtzw5un88GOXcrCxhT//+iN8a80WWttGXKkiMsyZezL7sONdPQvd/aZ4+nrgEndf2kvb7wI/d/f7+ljXEmAJQE1NzbwVK1YMqk/5fJ7KyspBffZE7G9q5+71zTy5q40ZVSlunFNK7eiTd3imWHUXU4g1Q5h1h1gzDLzuBQsWPOHudb0tGxZXFrv7cmA5QF1dndfX1w9qPblcjsF+9kRd+w7nF8++wm3/sZ4vPNrITZedySevmMmobPKboJh1F0uINUOYdYdYMwxt3Un+t3QHMLVgujaeFyQz45q5p/PQZy7n3RdN4ZtrtnDVV3/DL5/bSVKjMhGR/kgyCNYCs8xshpllgcXAygS/b1gYV5HlK9edz48//iYqSzN8/PtP8O47fsfvtuj2FCJSHIkFgbu3AkuBB4CNwL3uvt7MbjezRQBmdrGZbQfeC3zLzNYn1Z9TzcXTx/GLT76VL737jew80MgH/+9jfPjOx3hs655id01EApPoDmp3XwWs6jHvtoL3a4l2GQUpk06xeP4ZvOvCKXz/9y/xzTVbeP/y33Px9LH89YKZ1J81ETMrdjdFZITTlcWngLKSNDdddiaP/N3b+eKi89ix7ygf/fe1XP2v/8XPntpBi045FZEEKQhOIWUlaW5483Ryn13Av7z3fNranU//6Gku/8pqvvqrzby053CxuygiI9CwOH00NNlMiuvm1fLuC6ewetMuvvu7F/n6r5/n3x5+nounj+XdF9XyzjdOpqq8pNhdFZERQEFwCkuljCvOreGKc2v40/6j/PSpHfzkye3cev+z/MPK9Vx57iSunjOZ+rMnMrpMoSAig6MgGCZOry7nEwtm8tf1b+DZHQe4/8kd/PyZP7Hq2Z1k0ynePHM8V55bwxXnTmJyVXmxuysiw4iCYJgxM+bWVjO3tpq/v2Y2T768jwee28mDG17l85ue4/M/g/NOH8OCsydx2awJXDRtbLG7LCKnOAXBMJZOGRdPH8fF08fxuT87l4ZdeR7auItf//FV7lizhW+sbqCyNMNZVc6O8pd426yJTB03qtjdFpFTjIJghDAzZtWMZlbNaP5b/Rs4cLSFR7e8xprNr/HgM9v43E+fA+CMcaOomzaWuunjuPCMamZNqiST1sljIiFTEIxQVeUlLJwzmYVzJvOOsa8x9byLWbN5N2tf2Muazbu5/6notk+lmRSzTx/D3ClVnD+1mvOnVjNjfAWplC5kEwmFgiAAZsbMSZXMnFTJjW+dgbvzwmuHeWb7AZ7dEb1+/MR27nr0JQBGl2V445Qq5tZWM2fKGM45bTTTxldQopGDyIikIAiQmXHmxErOnFjJuy6cAkBbu9OwK88ftu3nD9v38+yOA3z7ka20tEV3Ri1JGzMmVDB9fEX0c0IFZ9VUMqtmNGN06qrIsKYgECA68Hz2aaM5+7TRvO/i6O7hTa1tPP9qnud3HWLTzjwNu/K88Nphcpt309zadduL08aUcebEKBzOnBAFxYwJFUwdN0qjCJFhQEEgfSrNpJkzpYo5U6q6zW9rd/60/yibXz3E5jgoXnjtML945hUOHG3pbJdOGadXlzFtXBQKtWPLqR1bzunV5UyuKmPS6DKyGQWFSLEpCGTA0ilj6rhRTB03iivOrem2bN/hZl7Yc5gXdh/mhdcO8/LeI7y09wi/fO4V9h1pOWZdEyqznF5dzpTqroA4raqM08ZEQTFxdCnl2fTJKk0kSAoCGVJjK7KMrchy0RnHXsh2pLmVP+0/yo79jew8cJSdB5rYeTCa3vzqIVZv2kVjy7F3Wh1dmmF8ZZbxlaWMq8gyoTLLhMpSJlSWMr4yy7iKLOMrovdjR2VJ64wnkQFREMhJMyqbYeak0cycNLrX5e7OgaMt7DzYyM4Djew+1MTufBO7Djax53Azew838fKeIzz18j72Hm6mvZcnfJrB2FFZymhh8sbfUVVe0vkaU15CdXkJ1aOiV1V5lqryDGPKShhdVkJZSUrPf5AgKQjklGFmVI/KUj0qyzmnjXndtm3tzr4jzezJN7PncBN7D3e8b2ZPvolNL+4gm0nx6sFotHHgaAuHGltfd50laaOyNMPoshIqSzNUlmaoKE1TWVbC6LJM9CrNUBG/RmXTlJdEr7JsmopsNG9UNk1FaYbSjIJFhodEg8DMFgL/CqSBO939Sz2WlwJ3A/OAPcD73f3FJPskI0M6ZZ27h+DYEUYut4f6+ku7zWtrdw4ebeHA0Rb2H21h35FmDjW2cvBoCwcbo6A41NjCwaOtHGluJd/Uymv5Zl7ccySa39ja7Wyp40lZNAoqK4nCoTSTIptJUZJOUZpJUR6HRnlJhrKSFGUlacpKUmTTaUpLojalmWheeUmabPz5bDpFSfwzkzYyqRSZlJFJG/ub2jlwtCX6rnRKFwZKvyQWBGaWBpYBVwHbgbVmttLdNxQ0uxHY5+4zzWwx8GXg/Un1ScKWTlnnMYzBam5t53BTFBJHW9o42txGY0sbR+L3+aZWjja3cbi5lSNN0c/GlnYa4+Utbe00t7XT1NrOnnwz2wvW0djSRmNrO2297fMaiNUPdr5NGZSkU/HLOt9HAWIF76PlmVQ0XZJOkU5Fbbp+RoGTjj+bsmhZOn6fTRuZeP3pFMcs73ifjt+n4vcpi265HrWJPtf1ipZF6+ha1nEcKGXGtkPtbH71UOf8jnbQsd5o2oxu6zUMi7/P6NkmGqEa0e7GkT6yS3JEMB9ocPetAGa2ArgWKAyCa4EvxO/vA75hZubuJ/gvQSQZ0f/KTyxMjqe1Iyxa2mlsbesMkubWaH7Hz9Y2p6WtnZa2KDxa25znNv6RaWfOpKm1jZZWp7U9at9S0LalLZrf8fnW9vhnPP9oi3eus/Pl3rm8rR3a2qPPtcfL2tq98+LDovjtbxL/io6QKAyNjnwwCgKoIHgK20LXPLOuz3SETG/r6wgj4s99+sqz+PPzTx/y2pIMginAtoLp7cAlfbVx91YzOwCMB14rbGRmS4AlADU1NeRyuUF1KJ/PD/qzw1mIdY/0mg0oiV+F5o9rorI1ulVI57/uIcksi199X/fh7rQ7tDq0xy/vfO+00zW/23Ki5Z1tieY7Pdfj3efF33vkaCOlpWUFn43adazDC9p3fJ4e3+P4Me2ha5rCz9LVbzqXF3w+rqfr81F/KFhOwXoLvyteU1dbL5wPLz2/kdy+zcDQ/h0fFgeL3X05sBygrq7O6+vrB7WeXC7HYD87nIVYd4g1Q5h1h1gzDG3dSV7WuQOYWjBdG8/rtY2ZZYAqooPGIiJykiQZBGuBWWY2w8yywGJgZY82K4Eb4vfXAb/W8QERkZMrsV1D8T7/pcADRKePfsfd15vZ7cA6d18JfBv4npk1AHuJwkJERE6iRI8RuPsqYFWPebcVvG8E3ptkH0RE5PXp1o8iIoFTEIiIBE5BICISOAWBiEjgbLidrWlmu4GXBvnxCfS4ajkQIdYdYs0QZt0h1gwDr3uau0/sbcGwC4ITYWbr3L2u2P042UKsO8SaIcy6Q6wZhrZu7RoSEQmcgkBEJHChBcHyYnegSEKsO8SaIcy6Q6wZhrDuoI4RiIjIsUIbEYiISA8KAhGRwAUTBGa20Mw2mVmDmd1S7P4kwcymmtlqM9tgZuvN7FPx/HFm9iszez7+ObbYfR1qZpY2s6fM7Ofx9Awzeyze3j+Kb4U+ophZtZndZ2Z/NLONZvamQLb1zfHf7+fM7B4zKxtp29vMvmNmu8zsuYJ5vW5bi/xbXPszZnbRQL8viCAwszSwDLgamA18wMxmF7dXiWgF/sbdZwOXAp+I67wFeNjdZwEPx9MjzaeAjQXTXwa+5u4zgX3AjUXpVbL+Ffilu58DnE9U/4je1mY2BfgkUOfuc4hucb+Ykbe9vwss7DGvr217NTArfi0B7hjolwURBMB8oMHdt7p7M7ACuLbIfRpy7v6Kuz8Zvz9E9IthClGtd8XN7gLeVZQOJsTMaoE/A+6Mpw14O3Bf3GQk1lwFvI3omR64e7O772eEb+tYBiiPn2o4CniFEba93f03RM9oKdTXtr0WuNsjvweqzWzyQL4vlCCYAmwrmN4ezxuxzGw6cCHwGFDj7q/Ei3YCNcXqV0L+N/C3dD1PfDyw391b4+mRuL1nALuBf493id1pZhWM8G3t7juAfwFeJgqAA8ATjPztDX1v2xP+/RZKEATFzCqBnwCfdveDhcviR4GOmHOGzewaYJe7P1HsvpxkGeAi4A53vxA4TI/dQCNtWwPE+8WvJQrC04EKjt2FMuIN9bYNJQh2AFMLpmvjeSOOmZUQhcAP3P3+eParHUPF+OeuYvUvAW8BFpnZi0S7/N5OtO+8Ot51ACNze28Htrv7Y/H0fUTBMJK3NcCVwAvuvtvdW4D7if4OjPTtDX1v2xP+/RZKEKwFZsVnFmSJDi6tLHKfhly8b/zbwEZ3/2rBopXADfH7G4D/ONl9S4q73+rute4+nWi7/trdPwSsBq6Lm42omgHcfSewzczOjmddAWxgBG/r2MvApWY2Kv773lH3iN7esb627UrgL+Kzhy4FDhTsQuofdw/iBbwT2AxsAT5X7P4kVONbiYaLzwBPx693Eu0zfxh4HngIGFfsviZUfz3w8/j9mcDjQAPwY6C02P1LoN4LgHXx9v4ZMDaEbQ18Efgj8BzwPaB0pG1v4B6iYyAtRKO/G/vatoARnRW5BXiW6IyqAX2fbjEhIhK4UHYNiYhIHxQEIiKBUxCIiAROQSAiEjgFgYhI4BQEIieRmdV33CFV5FShIBARCZyCQKQXZvZhM3vczJ42s2/FzzvIm9nX4nvhP2xmE+O2F5jZ7+N7wf+04D7xM83sITP7g5k9aWZviFdfWfAcgR/EV8iKFI2CQKQHMzsXeD/wFne/AGgDPkR0g7N17n4esAb4h/gjdwN/5+5zia7s7Jj/A2CZu58PvJnoSlGI7gr7aaJnY5xJdK8ckaLJHL+JSHCuAOYBa+P/rJcT3eCrHfhR3Ob7wP3xcwGq3X1NPP8u4MdmNhqY4u4/BXD3RoB4fY+7+/Z4+mlgOvBI4lWJ9EFBIHIsA+5y91u7zTT7+x7tBnt/lqaC923o36EUmXYNiRzrYeA6M5sEnc+KnUb076XjDpcfBB5x9wPAPjO7LJ5/PbDGoyfEbTezd8XrKDWzUSezCJH+0v9ERHpw9w1m9nngQTNLEd0B8hNED3+ZHy/bRXQcAaJbAn8z/kW/FfhoPP964Ftmdnu8jveexDJE+k13HxXpJzPLu3tlsfshMtS0a0hEJHAaEYiIBE4jAhGRwCkIREQCpyAQEQmcgkBEJHAKAhGRwP1/S7Add8qP3MEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), loss_epoch)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), acc_epoch)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Turn off gradients to speed up this part\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     logps \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(img)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Output of the network are log-probabilities, need to take exponential for probabilities\u001b[39;00m\n\u001b[1;32m      9\u001b[0m ps \u001b[38;5;241m=\u001b[39m softmax(logps)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = softmax(logps)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
